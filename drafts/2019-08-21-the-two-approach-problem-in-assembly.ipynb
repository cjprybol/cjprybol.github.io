{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post  \n",
    "title: The Two Approach Problem in Assembly  \n",
    "date: 2019-08-21  \n",
    "author: Cameron Prybol  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to read DNA there are two fundamental problems. The first is reading DNA sequences from beginning to end without any breaks. The second is reading the DNA without error. Collectively, these two problems are sufficient to support an entire domain of sequence assembly research.\n",
    "\n",
    "When sequencing DNA, our goal is to sequence the DNA contiguously, accurately, and efficiently. However, our current DNA sequencing technologies allow us to achieve two of these three goals at a time. The earliest form of DNA sequencing, known as [Sanger sequencing](https://en.wikipedia.org/wiki/Sanger_sequencing), is very accurate and can sequence DNA in relatively long stretches up to thousands of base-pairs, however it is low throughput and can be expensive at scale. The next major breakthrough in DNA sequencing, [Illumina sequencing](https://en.wikipedia.org/wiki/Illumina_dye_sequencing), allows for very accurate and high throughput sequencing but cannot read more than a few hundred basis at a time. Newer forms of sequencing, such as [nanopore sequencing](https://en.wikipedia.org/wiki/Nanopore_sequencing), are able to read very long stretches of DNA and with high throughput, however they can be 1 to 2 orders of magnitude less accurate then either Sanger or Illumina sequencing.\n",
    "\n",
    "Ideally, we could have a generalized, probabilistic approach to generating accurate and contiguous DNA sequence assemblies that works with any input sequencing technology (although my strong bias is towards nanopore sequencing!)\n",
    "\n",
    "The predominant methodology of DNA sequence assembly used for Sanger and Illumina sequencing utilizes [De Bruijn graphs](https://en.wikipedia.org/wiki/De_Bruijn_graph). Some relevant links:\n",
    "- [Ben Langmead's notes](https://www.cs.jhu.edu/~langmea/resources/lecture_notes/assembly_dbg.pdf)\n",
    "- [Velvet Assembler](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2336801/)\n",
    "- [Velvet Wikipedia page](https://en.wikipedia.org/wiki/Velvet_assembler)\n",
    "- [Pevzner's Euler Paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC55524/)\n",
    "- [Stanford Course Notes](https://data-science-sequencing.github.io/Win2018/lectures/lecture7/)\n",
    "- [Colored De Bruijn Graphs](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3272472/).\n",
    "\n",
    "Techniques developed for the new longer lower accuracy reads generally utilize an overlap-layout-consensus type approach. Some relevant links:\n",
    "- [Flye](https://sci-hub.tw/https://www.nature.com/articles/s41587-019-0072-8)\n",
    "- [Canu](https://genome.cshlp.org/content/27/5/722)\n",
    "- [Ra](https://github.com/lbcb-sci/ra)\n",
    "\n",
    "Without getting into too much detail but how either of these approaches work (follow the above links to learn more), De Bruijn graph assembly requires high accuracy and existing implementations work best with Illumina datasets which lack the long-range sequence information necessary to properly resolve repetitive regions, whereas the overlap layout consensus approach can tolerate the higher error rates of long-read sequencing technology which enables resolving repetitive regions and achieve accurate long-range structure but can result in lots of minor errors that have a big impact on gene prediction. See some great articles by Mick Watson on this issue:\n",
    "- [Mind the gaps – ignoring errors in long read assemblies critically affects protein prediction](https://www.biorxiv.org/content/10.1101/285049v1)\n",
    "- [A simple test for uncorrected insertions and deletions (indels) in bacterial genomes](http://www.opiniomics.org/a-simple-test-for-uncorrected-insertions-and-deletions-indels-in-bacterial-genomes/)\n",
    "- [On stuck records and indel errors; or “stop publishing bad genomes”](http://www.opiniomics.org/on-stuck-records-and-indel-errors-or-stop-publishing-bad-genomes/).\n",
    "\n",
    "Depending on context the relative strengths and weaknesses of these approaches may make one significantly more preferable to the other. For example, when trying to assemble a genome from scratch, long reads with higher error rates tend to produce more complete drafts that can be improved with statistical approaches and the addition of higher-accuracy short-reads. In the context of clinical medicine, where we often care much more about the accuracy of our genome assembly within protein coding genes, and specifically genetic variations in protein coding genes that may impair their function and cause disease, we're willing to forgo the long-range information necessary to span repetitive, non-protein coding regions of the genome in exchange for very accurate results in these protein coding regions of interest.\n",
    "\n",
    "The point that I'm trying to get to is that we shouldn't need to choose between these two approaches. By utilizing modern hi-throughput sequencing technologies that retain long-range sequence information and time-tested information theory approaches that'll allow us to find the signal in the noise to produce accurate assemblies, we can have our cake and eat it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
