When trying to read DNA there're two fundamental problems. The first problem is reading the DNA input from beginning to end without any gaps or breaks. The second problem is reading the DNA without making any mistakes or errors during the reading process. Collectively these two problems are the basis for a research discipline known generally as sequences assembly. When sequencing DNA our goal is to be able to sequence the DNA accurately quickly and contiguously, however, how DNA sequencing technologies generally allow us to choose two of these three goals. The earliest form of DNA sequencing, known as Sanger sequencing is very accurate and can sequence DNA in relatively long stretches up to thousands of basepairs, however it is low throughput and can be expensive. The next major breakthrough in DNA sequencing has been commercialized by Illumina and allows four very accurate sequencing and very high throughput sequencing but cannot read more than a few hundred basis at a time. New were forms of sequencing such as those developed by Pacific biosciences and Oxford nano for are able to read very long stretches of DNA and so with relatively high throughput however they can be 1 to 2 orders of magnitude less accurate Dan either Sanger or alumina sequencing. Ideally we could have a completely generalized probabilistic approach to generating highly accurate and contiguous DNA sequencing assemblies regardless of the input sequencing technology. Currently, this is not the case. The predominant form of DNA sequence assembly realized Forest anger sequencing and luminous sequencing utilizes a technique known as debruyn graph assembly. Whereas techniques developed for the new longer lower accuracy reads generally utilize in an overlap layout consensus type approach. Linked to papers describing both of these approaches. Without getting into too much detail but How either of these approaches work, to brewing graph assembly generally requires very high accurate accuracy and tends to struggle with retaining long-rang sequence information because repetitive sequences get compressed in the Assembly process and the high accuracy sequencing Reese in the high accuracy sequencing technologies that work best with this assembly approach do not retain enough Long range sequence information to disambiguate long-range continuity information. The overlap layout consensus approach in contrast does a very good job retaining the long-range information but tends to suffer in overall assembly accuracy both in terms of Single nucleotide Errors and more importantly insertion deletion errors. Depending on context the relative strengths and weaknesses of these approaches may make one significantly more preferable to the other. For example when trying to assemble the genome of an organism that has never had a genome assembly before utilizing Long reads with higher error rates and utilizing the overlap layout consensus approach enables creating relatively accurate giraffes whole genome assembly that can be improved upon or "polished "buy aligning utilizing probabilistic approaches and higher accuracy rate technologies however in the context human clinical medicine where we often care much more about the accuracy of our genome assembly of a specific patient within the context of there're protein coding genes that may influence their health and be a significant drivers in disease symptoms that they maybe presenting with in front of a healthcare provider we may be far more likely to tolerate losing Long range genome information about the specific ordering in order to ensure that our assembly accuracy of the jeans themselves accurate to promote optimal healthcare decision-making buy care providers. Of course this creates the issue where we clinical genome research misses larger insertions, deletions, and genome rearrangements that may have huge effect sizes on patient health whereas long-read de novo genome assemblies may incorrectly assemble protein coding genes that cause us to infer that an entire protein coding gene is non-functional due to what we incorrectly infer is a major amino acid substitution, early stop codon, or frame shift.

the point that I'm trying to get two that we don't need to choose between these two approaches. By utilizing modern hi throughput and sequencing technologies that retain long contiguous sequence information genomes and utilizing time-tested information theory approaches that'll allow us to find the signal in the sequencing noise to produce highly accurate assemblies without the need for it's from other technologies we can have our cake and eat it too with the Novo genome assembly it gives us high accuracy assemblies the first time every time using existing Technology and sequencing capability.