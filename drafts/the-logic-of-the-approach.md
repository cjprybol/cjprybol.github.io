In the previous post I laid out the two approached problem currently in genomics and touched on what I believe is the solution. My goal here is to layout the approach that I think Best addresses this problem and best implements he proposed solution.

Given the inherent limitations of short read alumina technology in that the technology is unable to retain the long read given that the technology is unable to retain the long-distance information to resolve repeats improperly order sequences and genomes, and am not currently convinced that technological"Band-Aids" such as those currently developed by 10 ask Jenelle Max and others Will be the appropriate long-term solutions even though I acknowledge they have radically improves our current capabilities. Therefore focusing exclusively on the hi throughput Long read technologies that generally produce slower accuracy outputs specifically Oxford mental core technology given its rapidly dropping costs and potential for distributed use in the fields and by those with limited resource access, the goal is to have an approach that allows us to get read utilizing this technology as accurate as or more accurate than Illumina reads. I also think that the tools techniques and knowledge necessary to do this already exist, and it's just a matter of Putting them together and creating a medical minimal viable prototype to demonstrate utility and by the ability of this approach.

The steps are as follows:
One. Sequence a DNA sample of interest to the greatest amounts financially viable
Two. Create a Debruin assembly graph from the reads
three. Utilize the Debruin graph as a probabilistic model and evaluate each read iteratively through the probabilistic model and utilize the Viterbi algorithm to find the highest likelihood sequence of DNA that could have been sequenced and be the underlying True sequence of DNA that generated the observed read.
four. repeat steps 2 and 3 with the output "corrected" sequences from step 3 until no additional corrections are observed.

This is effectively an expectation maximization approach to generating the most likely sequence assembly possible given the input DNA. The key features of this approach are that we don't need to make any assumptions except for the underlying accuracy of the sequencing technology itself which can be measured to a very high accuracy and precision before hand using a curated reference library to evaluate accuracy and to enable calibration. The two biggest weaknesses of this approach are that approaches like the Viterbi algorithm that consider every possible state can be very very and potentially intractably slow on problems as complex as DNA assembly and because the runs speed of the turkey algorithm is dependent on and because we will be utilizing relatively low accuracy reads we expect that during the first iteration of this approach the number of's Various erroneous kmers will far outnumber the True or accurate kaymer's in the model, it may turn out that this approach simply cannot work given today's computing speed and power. However given the ubiquity of cheap affordable and scalable computer power and the availability of GPU approaches that can speed up the probability calculations behind the paternity algorithm by orders of magnitude I do not believe that this will actually be a showstopping problem although it could prevents lower value uses of this approach from being viable in today's market and would results in those domains not being explored future conditions I'm more conducive. Another big consideration about the tractability of this approach can be addressed by the approach itself. Specifically based on the stability of the kier counts â€“ kier frequency relationships demonstrated across all organisms it seems reasonable that this approach can be applied starting at relatively low k length values that would remove most errors while the number of K murs is still small and tractable there bye reducing and bounding the probabilistic search space for the for Terbi algorithm significantly, before incrementally repeating the assembly and correction process utilizing progressively longer K length values that allow for the production of smoother assemblies that resolve more repeats and enhance our understanding of long-range information.